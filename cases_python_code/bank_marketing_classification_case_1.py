# -*- coding: utf-8 -*-
"""bank_marketing_classification_case_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UXn6OC7s-Fm2wvsCsbej1XV3-mponkIp
"""

# install packages here

# Commented out IPython magic to ensure Python compatibility.
# Import required packages

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import missingno as msno
from tabulate import tabulate
from scipy import stats
from scipy.stats import chi2_contingency
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression # logistic regression
from sklearn.svm import SVC # support vector Machine
from sklearn.ensemble import RandomForestClassifier # Random Forest
from sklearn.neighbors import KNeighborsClassifier # KNN
from sklearn.tree import DecisionTreeClassifier # Decision Tree
from sklearn.model_selection import train_test_split # training and testing data split
from sklearn import metrics # accuracy measure
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix # confusion matrix
from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, log_loss, confusion_matrix, classification_report

sns.set_theme(style="whitegrid")

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

# Load dataset
from google.colab import drive

drive.mount('/content/drive')

path = r'/content/drive/MyDrive/datasets/bank-additional-full.csv'

df=pd.read_csv(path, sep=';', index_col=False)

df.head()

df.tail()

df.info()

df.columns

df.shape

df.dtypes

# Change object data type to categorical data type
df = df.astype({'job':'category', 'marital':'category', 'education':'category', 'default':'category', 'housing':'category', 'loan':'category', 'contact':'category', 'month':'category', 'day_of_week':'category', 'poutcome':'category', 'y':'category'})

df.dtypes

# Change column names
new_column_names = {
    'contact': 'contact_type',
    'duration': 'last_contact_duration',
    'pdays': 'passed_days',
    'poutcome': 'previous_outcome',
    'emp.var.rate': 'employment_variation_rate',
    'cons.price.idx': 'consumer_price_index',
    'cons.conf.idx': 'consumer_confidence_index',
    'nr.employed': 'number_of_employees'
}

# Rename the columns
df.rename(columns=new_column_names, inplace=True)

# Create a column called has_term_deposit which is going to represent 0 for the negative class and 1 for the positive class.
df['has_term_deposit'] = (df.y == 'yes').astype('int')

df.describe().T

df.isnull().sum()

# Check the duplicated rows
df.duplicated().sum()

# Print duplicated values

"""
Duplicated rows can introduce bias and skew the analysis results. By removing duplicated rows, analysts can ensure the accuracy and integrity of the data, leading to more reliable insights and conclusions.
"""

duplicated_rows = df[df.duplicated(keep=False)]  # keep=False marks all occurrences as True
duplicated_rows

"""
For duplicated values, we can do this:
    - Review and Understand.
    - Address Data Entry Issues.
    - Aggregate Duplicates.
    - Flag Duplicates.
    - Handle Partial Duplicates.
    - Identify and Remove Duplicates.
"""

df = df.drop_duplicates()

df.duplicated().sum()

# Check the missing values
df.isnull().sum()

df.shape

# Explore unique values and their counts in a specified column

def explore_column(df, column_name):

    unique_values = df[column_name].unique()
    value_counts = df[column_name].value_counts()

    return unique_values, value_counts

unique_values, value_counts = explore_column(df, 'age')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'job')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'marital')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'education')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'default')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'housing')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'loan')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'contact_type')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'month')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'day_of_week')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'last_contact_duration')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'campaign')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'passed_days')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'previous')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'previous_outcome')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'employment_variation_rate')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'consumer_price_index')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'consumer_confidence_index')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'euribor3m')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'number_of_employees')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

# Plot the (target) customers subscriptions to a term deposit

plt.figure(figsize=(20, 10))

counts = df['y'].value_counts()
labels = counts.index

plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])
plt.title('Distribution of customers who are subscribed or unsubscribed to a term deposit', fontsize=14, fontweight='bold')
plt.show()

# Plot customers subscriptions to a term deposit (Using plotly)
fig = px.pie(df, names='y', title='Distribution of customers who are subscribed or unsubscribed to a term deposit')
fig.show()

"""ðŸ‘†ðŸ» The dataset is imbalanced."""

# Plot the missing values
msno.bar(df)

# Plot the missing values
msno.matrix(df)

df[list(df.columns)[:12]].sample(10)

df[list(df.columns)[12:]].sample(10)

# Plot outliers for numerical variables using box plot

def plot_numerical_vars_with_outliers(df, column_name):
    plt.rcParams['axes.facecolor'] = '#f8fafc'
    plt.figure(figsize=(18, 10))

    sns.boxplot(x=df[column_name], palette='Paired')
    plt.title('Box Plot of Numerical Variables with Outliers')
    plt.show()

# Plot outliers for numerical variables using box plot (Plotly package)
def plot_numerical_vars_with_outliers_plotly(df, column_name, background_color='#f8fafc'):
    fig = go.Figure()

    # Add box plot
    fig.add_trace(go.Box(x=df[column_name], marker=dict(color='#f43f5e')))

    # Add mean as dotted line
    mean_value = df[column_name].mean()
    fig.add_trace(go.Scatter(x=[mean_value, mean_value], y=[0, 1], mode='lines', line=dict(dash='dash', color='black'), name='Mean'))

    # Update layout
    fig.update_layout(
        title=f'Box Plot of Numerical Variable {column_name} with Outliers',
        paper_bgcolor=background_color
    )

    fig.show()

# Plot distribution of categorical variables.
def plot_categorical_vars_distribution(df, column_name, background_color='#f8fafc', palette='pastel'):
    # Set background color
    plt.rcParams['axes.facecolor'] = background_color

    # Calculate percentage values
    total_count = len(df)
    percentage_values = (df[column_name].value_counts() / total_count) * 100

    # Create count plot
    plt.figure(figsize=(18, 10))
    ax = sns.countplot(x=df[column_name], palette=palette)

    # Add percentage labels and borders
    for p in ax.patches:
        percentage = f'{p.get_height() / total_count * 100:.2f}%'
        x = p.get_x() + p.get_width() / 2
        y = p.get_height()
        ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=10, color='black')

    # Customize the plot
    plt.title(f'Count Plot of Categorical Variable - {column_name}')
    plt.xticks(rotation=90)
    sns.despine(left=True, bottom=True)  # Remove spines on the left and bottom

    # Show the plot
    plt.show()

# Plot distribution of categorical variables (Using Plotly)
def plot_categorical_vars_distribution_plotly(df, column_name, background_color='#f8fafc'):
    # Calculate percentage values
    total_count = len(df)
    percentage_values = (df[column_name].value_counts() / total_count) * 100

    # Create a DataFrame with count and percentage values
    count_df = df[column_name].value_counts().reset_index()
    count_df.columns = [column_name, 'count']
    count_df['percentage'] = (count_df['count'] / total_count) * 100

    # Create a bar plot using Plotly
    fig = px.bar(count_df, x=column_name, y='count', color=column_name,
                 labels={'x': column_name, 'y': 'Count'}, title=f'Count Plot of Categorical Variable - {column_name}',
                 color_discrete_sequence=px.colors.qualitative.Pastel)

    # Add percentage labels and borders
    for bar in fig.data:
        percentage = f'{count_df[count_df[column_name] == bar.name]["percentage"].values[0]:.2f}%'
        bar.update(text=percentage, hoverinfo='text')

    # Customize the layout
    fig.update_layout(
        paper_bgcolor=background_color,
        xaxis_tickangle=-45
    )

    # Show the plot
    fig.show()

plot_numerical_vars_with_outliers(df, 'age')

plot_numerical_vars_with_outliers_plotly(df, 'age')

plot_numerical_vars_with_outliers(df, 'last_contact_duration')

plot_numerical_vars_with_outliers_plotly(df, 'last_contact_duration', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'campaign')

plot_numerical_vars_with_outliers_plotly(df, 'campaign', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'passed_days')

plot_numerical_vars_with_outliers_plotly(df, 'passed_days', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'previous')

plot_numerical_vars_with_outliers_plotly(df, 'previous', '#E6ECF5')

"""### Handling outliers"""

# Create a copy of the dataset
df2 = df.copy()

# Using Z-scores [Count the number of outliers in a specific column of a dataset using Z-scores]

def detect_count_outliers(df, column_name, threshold=3):

    # Calculate Z-scores for the specified column
    z_scores = stats.zscore(df[column_name])

    # Identify outliers
    outliers_df = df[abs(z_scores) > threshold]

    # Count the number of outliers
    num_outliers = len(outliers_df)

    print(f"Number of outliers in '{column_name}': {num_outliers}")

    return num_outliers, outliers_df

# Handle outliers by replacing them with the median.
def handle_outliers_with_median(df, column_name, threshold=3):

    # Calculate Z-scores for the specified column
    z_scores = stats.zscore(df[column_name])

    # Replace outliers with the median
    df.loc[abs(z_scores) > threshold, column_name] = df[column_name].median()

    return df

num_outliers, outliers_df = detect_count_outliers(df2, 'age', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'age', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'age', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'age')

plot_numerical_vars_with_outliers_plotly(df2, 'age')

num_outliers, outliers_df = detect_count_outliers(df2, 'last_contact_duration', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'last_contact_duration', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'last_contact_duration', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'last_contact_duration')

plot_numerical_vars_with_outliers_plotly(df2, 'last_contact_duration', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'campaign', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'campaign', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'campaign', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'campaign')

plot_numerical_vars_with_outliers_plotly(df2, 'campaign', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'passed_days', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'passed_days', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'passed_days', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'passed_days')

plot_numerical_vars_with_outliers_plotly(df2, 'passed_days', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'previous', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'previous', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'previous', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'previous')

plot_numerical_vars_with_outliers_plotly(df2, 'previous', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'employment_variation_rate', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'employment_variation_rate')

num_outliers, outliers_df = detect_count_outliers(df2, 'consumer_price_index', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'consumer_price_index')

num_outliers, outliers_df = detect_count_outliers(df2, 'consumer_confidence_index', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'consumer_confidence_index')

num_outliers, outliers_df = detect_count_outliers(df2, 'euribor3m', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'euribor3m')

num_outliers, outliers_df = detect_count_outliers(df2, 'number_of_employees', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'number_of_employees')

"""### Data Visualization"""

# See the correlations between categorical variables
def plot_categorical_count(df, x_column, hue_column, title, palette='Paired'):

    plt.figure(figsize=(20, 10))
    sns.countplot(x=x_column, hue=hue_column, data=df, palette=palette)

    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel(x_column)
    plt.ylabel('Count')
    plt.legend(title=hue_column, loc='upper right')

    plt.show()

# See the correlations between categorical variables (Using Plotly)
def plotly_categorical_count(df, x_column, hue_column, title, color_discrete_sequence=None):

    # Create a dataframe with counts and percentages
    count_df = df.groupby([x_column, hue_column]).size().reset_index(name='count')
    total_counts = count_df.groupby(x_column)['count'].transform('sum')
    count_df['percentage'] = (count_df['count'] / total_counts) * 100

    # Create the bar plot
    fig = px.bar(count_df, x=x_column, y='count', color=hue_column,
                 title=title, color_discrete_sequence=color_discrete_sequence)

    # Add borders and percentage labels
    for bar in fig.data:
        bar.update(marker_line_color='black', marker_line_width=1, hoverinfo='y+text',
                   text=count_df[count_df[hue_column] == bar.name]['percentage'].round(2).astype(str) + '%')

    fig.update_layout(
        xaxis_title=x_column,
        yaxis_title='Count',
        legend_title=hue_column,
        barmode='group',
        bargap=0.1,
        showlegend=True,
        legend=dict(x=1.02, y=1)
    )

    fig.show()

df2.groupby(['job','y'])['y'].count()

pd.crosstab(df2.y, df2.job, margins=True).style.background_gradient(cmap='summer_r')

plot_categorical_count(df2[['job', 'y']], x_column='job', hue_column='y', title='Count Plot of Job vs. Subscription', palette='PuRd')

# Plotly
plotly_categorical_count(df2[['job', 'y']], x_column='job', hue_column='y', title='Count Plot of Job vs. Subscription')

df2.groupby(['marital','y'])['y'].count()

pd.crosstab(df2.y, df2.marital, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.job, df2.y], df.marital, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between job, subscription and marital status

cross_tab_df = pd.crosstab([df2['job'], df2['y']], df2['marital'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Job, Subscription, and Marital Status', fontweight='bold', fontsize=16)
plt.xlabel('Marital Status', fontweight='bold', fontsize=14)
plt.ylabel('Job and Subscription', fontweight='bold', fontsize=14)
plt.show()

plot_categorical_count(df2[['marital', 'y']], x_column='marital', hue_column='y', title='Count Plot of Marital status vs. Subscription', palette='Reds')

# Plotly
plotly_categorical_count(df2[['marital', 'y']], x_column='marital', hue_column='y', title='Count Plot of Marital status based on Subscription')

df2.groupby(['education','y'])['y'].count()

pd.crosstab(df2.y, df2.education, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.job, df2.y], df2.education, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between job, subscription and education

cross_tab_df = pd.crosstab([df2['job'], df2['y']], df2['education'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Job, Subscription, and Education', fontweight='bold', fontsize=16)
plt.xlabel('Education', fontweight='bold', fontsize=14)
plt.ylabel('Job and Subscription', fontweight='bold', fontsize=14)

plt.show()

plot_categorical_count(df2[['education', 'y']], x_column='education', hue_column='y', title='Count Plot of Education based on Subscription', palette='PuRd')

# Plotly
plotly_categorical_count(df2[['education', 'y']], x_column='education', hue_column='y', title='<b>Count Plot of Education based on Subscription</b>')

df2.groupby(['default','y'])['y'].count()

pd.crosstab(df2.y, df2.default, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.age, df2.y], df2.default, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between age, subscription and default
cross_tab_df = pd.crosstab([df2['age'], df2['y']], df2['default'], margins=True)

plt.figure(figsize=(20, 25))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Age, Subscription, and Default(has credit in default?)', fontweight='bold', fontsize=16)
plt.xlabel('Education', fontweight='bold', fontsize=14)
plt.ylabel('Job and Subscription', fontweight='bold', fontsize=14)

plt.show()

plot_categorical_count(df2[['default', 'y']], x_column='default', hue_column='y', title='Count Plot of -Has credit in default? based on Subscription', palette='Set3')

# Plotly
plotly_categorical_count(df2[['default', 'y']], x_column='default', hue_column='y', title='Count Plot of - Has credit in default? based on Subscription')

df2.groupby(['housing','y'])['y'].count()

pd.crosstab(df2.y, df2.housing, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.marital, df2.y], df2.housing, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between marital, subscription and housing
cross_tab_df = pd.crosstab([df2['marital'], df2['y']], df2['housing'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Marital, Subscription, and Housing (has housing loan?)', fontweight='bold', fontsize=16)
plt.xlabel('Housing', fontweight='bold', fontsize=14)
plt.ylabel('Marital and Subscription', fontweight='bold', fontsize=14)

plt.show()

plot_categorical_count(df2[['housing', 'y']], x_column='housing', hue_column='y', title='Count Plot of Housing loan based on Subscription', palette='Wistia')

# Plotly
plotly_categorical_count(df2[['housing', 'y']], x_column='housing', hue_column='y', title='Count Plot of Housing loan based on Subscription')

df2.groupby(['loan','y'])['y'].count()

pd.crosstab(df2.y, df2.loan, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.loan, df2.y], df2.marital, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between loan, subscription and marital status
cross_tab_df = pd.crosstab([df2['loan'], df2['y']], df2['marital'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Loan (has personal loan?), Subscription, and Marital', fontweight='bold', fontsize=16)
plt.xlabel('Marital', fontweight='bold', fontsize=14)
plt.ylabel('Loan and Subscription', fontweight='bold', fontsize=14)

plt.show()

plot_categorical_count(df2[['loan', 'y']], x_column='loan', hue_column='y', title='Count Plot of Personal loan based on Subscription', palette='brg')

# Plotly
plotly_categorical_count(df2[['loan', 'y']], x_column='loan', hue_column='y', title='<b>Count Plot of Personal loan based on Subscription</b>')

# Select the specified features
numeric_features = df2[['age', 'last_contact_duration', 'campaign', 'passed_days', 'previous',
            'employment_variation_rate', 'consumer_price_index', 'consumer_confidence_index',
            'euribor3m', 'number_of_employees', 'has_term_deposit']]

# Calculate the correlation matrix
correlation_matrix = numeric_features.corr()

# Set up the matplotlib figure
plt.figure(figsize=(20, 10))

# Plot the correlation matrix using a heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

# Display the plot
plt.title('Correlation Matrix', fontweight='bold', fontsize=14)

plt.show()

numeric_features = df2[['age', 'last_contact_duration', 'campaign', 'passed_days', 'previous',
            'employment_variation_rate', 'consumer_price_index', 'consumer_confidence_index',
            'euribor3m', 'number_of_employees', 'has_term_deposit']]

# Calculate the correlation matrix
correlation_matrix = numeric_features.corr()

# Create a Plotly figure for the correlation matrix
fig = px.imshow(correlation_matrix,
                labels=dict(x="Features", y="Features", color="Correlation"),
                x=numeric_features.columns,
                y=numeric_features.columns,
                color_continuous_scale='blues',  # Choose an appropriate color scale
                title='Correlation Matrix')

# Customize the layout
fig.update_layout(width=1000, height=800)

# Show the figure
fig.show()

# Plot a correlation matrix heatmap between one numeric feature and target
def plot_correlation_heatmap(df, features, target, cmap='coolwarm', figsize=(10, 8)):
    df_selected = df[features + [target]]

    # Calculate correlation matrix
    correlation_matrix = df_selected.corr()

    # Set up the matplotlib figure
    plt.figure(figsize=figsize)

    # Create a heatmap using seaborn
    sns.heatmap(correlation_matrix, annot=True, cmap=cmap, fmt='.2f', linewidths=0.5)

    # Set the title
    plt.title(f'Correlation Matrix Heatmap - {target} vs {", ".join(features)}')

    # Show the plot
    plt.show()

# Plot a correlation matrix heatmap between one numeric feature and target (Using Plotly)
def plot_correlation_heatmap_plotly(df, features, target, colorscale='Viridis'):
    # Select relevant columns
    df_selected = df[features + [target]]

    # Calculate correlation matrix
    correlation_matrix = df_selected.corr()

    # Create a heatmap using Plotly
    heatmap = go.Figure(data=go.Heatmap(
        z=correlation_matrix.values,
        x=correlation_matrix.columns,
        y=correlation_matrix.index,
        colorscale=colorscale,
        colorbar=dict(title='Correlation'),
    ))

    # Customize the layout
    heatmap.update_layout(
        title=f'Correlation Matrix Heatmap - {target} vs {", ".join(features)}',
        xaxis=dict(title='Variable'),
        yaxis=dict(title='Variable'),
    )

    # Show the plot
    heatmap.show()

plot_correlation_heatmap(df2, features=['age'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['age'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['last_contact_duration'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['last_contact_duration'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['campaign'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['campaign'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['passed_days'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['passed_days'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['previous'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['previous'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['employment_variation_rate'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['employment_variation_rate'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['consumer_price_index'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['consumer_price_index'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['consumer_confidence_index'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['consumer_confidence_index'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['euribor3m'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['euribor3m'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['number_of_employees'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['number_of_employees'], target='has_term_deposit')

plot_categorical_vars_distribution(df2, 'job')

plot_categorical_vars_distribution_plotly(df2, 'job', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'marital')

plot_categorical_vars_distribution_plotly(df2, 'marital', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'education')

plot_categorical_vars_distribution_plotly(df2, 'education', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'default')

plot_categorical_vars_distribution_plotly(df2, 'default', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'housing')

plot_categorical_vars_distribution_plotly(df2, 'housing', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'loan')

plot_categorical_vars_distribution_plotly(df2, 'loan', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'contact_type')

plot_categorical_vars_distribution_plotly(df2, 'contact_type', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'month')

plot_categorical_vars_distribution_plotly(df2, 'month', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'day_of_week')

plot_categorical_vars_distribution_plotly(df2, 'day_of_week', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'previous_outcome')

plot_categorical_vars_distribution_plotly(df2, 'previous_outcome', '#E6ECF5')

# Distribution of the Target
plot_categorical_vars_distribution(df2, 'y')

# Distribution of the Target
plot_categorical_vars_distribution_plotly(df2, 'y', '#E6ECF5')

print('=' * 60)
print('=' * 20, 'Summary statistics', '=' * 20)
print('=' * 60)

# Summary statistics
summary_stats = numeric_features.describe()

# Mean
mean_values = numeric_features.mean()

# Median
median_values = numeric_features.median()

# Standard Deviation
std_dev_values = numeric_features.std()

# Correlation matrix
correlation_matrix = numeric_features.corr()

# Display the results using tabulate
print("\nSummary Statistics:")
print(tabulate(summary_stats, headers='keys', tablefmt='pretty'))

print("\nMean Values:")
print(tabulate(mean_values.to_frame(), headers='keys', tablefmt='pretty'))

print("\nMedian Values:")
print(tabulate(median_values.to_frame(), headers='keys', tablefmt='pretty'))

print("\nStandard Deviation:")
print(tabulate(std_dev_values.to_frame(), headers='keys', tablefmt='pretty'))

print("\nCorrelation Matrix:")
print(tabulate(correlation_matrix, headers='keys', tablefmt='pretty'))

df2.describe().T

df2.isnull().sum()

df2.shape

df2.dtypes

"""### Convert Categorical features to Numeric features"""

# Create a copy of the dataset
df3 = df2.copy()

LE=LabelEncoder()

cat_var=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact_type', 'month', 'day_of_week', 'previous_outcome', 'y']

for i in cat_var:
    df3[i]=LE.fit_transform(df3[i])

# Commented out IPython magic to ensure Python compatibility.
# To see all the variables created globally across the notebook.
# %whos

# Split the Data into Validation and Training Sets
X = df3.drop(['has_term_deposit', 'y'], axis=1)
y = df3['has_term_deposit']

sc = StandardScaler()
sc.fit_transform(X)

# Handling the imbalanced data by Resampling
sm = SMOTE(random_state = 42)
X_resampled, y_resampled = sm.fit_resample(X, y)

# This code first splits the dataset into a training set (70%) and a temporary set (30%). Then, it splits the temporary set into testing and validation sets, resulting in the desired sizes for each set.
# The final splits:
# Training set size = 70%
# Testing set size = 15%
# Validation set size = 15%

# Dataset splitting into training and temporary set (combining testing and validation)
X_train_temp, X_temp, y_train_temp, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Now, split the temporary set into testing and validation
# testing
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# validation
X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.5, random_state=42)

X_train.shape

X_test.shape

X_val.shape

y_train.shape

y_test.shape

y_val.shape

"""### Model Selection

  * Logistic regression.
  * SVM - Support Vector Machine.
  * KNN - K Nearest Neighbors.
  * Decision tree.
  * Random forest.

"""

def calc_popularity(y_actual):
    return (sum(y_actual)/len(y_actual))

# Calculates specificity
def calc_specificity(y_actual, y_pred, thresh):
    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)

# Evaluation of model performance
def print_report(y_actual, y_pred, thresh):
    auc = roc_auc_score(y_actual, y_pred)
    accuracy = accuracy_score(y_actual, (y_pred > thresh))
    recall = recall_score(y_actual, (y_pred > thresh))
    precision = precision_score(y_actual, (y_pred > thresh))
    specificity = calc_specificity(y_actual, y_pred, thresh)
    f1 = 2 * (precision * recall) / (precision + recall)
    logloss = log_loss(y_actual, y_pred)


    tn, fp, fn, tp = confusion_matrix(y_actual, (y_pred > thresh)).ravel()

    print('AUC: %.3f' % auc)
    print('Accuracy: %.3f' % accuracy)
    print('Recall: %.3f' % recall)
    print('Precision: %.3f' % precision)
    print('Specificity: %.3f' % specificity)
    print('Popularity: %.3f' % calc_popularity(y_actual))
    print('F1: %.3f' % f1)
    print('Log Loss: %.3f' % logloss)


    print('-'*20)
    confusion_matrix_table = tabulate([['True Negative', tn], ['False Positive', fp], ['False Negative', fn], ['True Positive', tp]],
                                     headers=['Metric', 'Value'], tablefmt='grid')
    print('---------- Confusion Matrix ----------\n')
    print(confusion_matrix_table)

    print(' ')
    return auc, accuracy, recall, precision, specificity, f1, logloss

# Since we balanced our training data, let's set our threshold at 0.5 to label a predicted sample as positive.

thresh = 0.5

"""### Logistic Regression"""

# Logistic regression
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

lr.random_state

# Training Set Estimation
y_pred_lr_train = lr.predict(X_train)
y_pred_lr_train

# Validation Set Estimation
y_pred_lr_val = lr.predict(X_val)
y_pred_lr_val

# Training Probabilities
y_pred_lr_train_Proba = lr.predict_proba(X_train)
y_pred_lr_train_Proba

# Validation Probabilities
y_pred_lr_val_Proba = lr.predict_proba(X_val)
y_pred_lr_val_Proba

# Training Classification Report
print("================== Logistic Regression Training Classification Report ==================\n")
print(classification_report(y_train, y_pred_lr_train))

# Evaluate the model on the training set
print('=' * 47)
print('======== Logistic Regression Training ========')
print('=' * 47)

thresh = 0.5

lr_train_auc, lr_train_accuracy, lr_train_recall, lr_train_precision, lr_train_specificity, lr_train_f1, lr_train_log_loss = print_report(y_train, y_pred_lr_train, thresh)

# Validation Classification Report
print("================== Logistic Regression Validation Classification Report ==================\n")
print(classification_report(y_val, y_pred_lr_val))

# Evaluate the model on the validation set
print('=' * 48)
print('======== Logistic Regression Validation ========')
print('=' * 48)

thresh = 0.5

lr_val_auc, lr_val_accuracy, lr_val_recall, lr_val_precision, lr_val_specificity, lr_val_f1, lr_val_log_loss = print_report(y_val, y_pred_lr_val, thresh)

# AUC-ROC curve Training Set
logit_roc_auc = roc_auc_score(y_train, y_pred_lr_train)
fpr, tpr, thresholds = roc_curve(y_train, lr.predict_proba(X_train)[:, 1])

plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
plt.title('Receiver operating characteristic - Training', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')
plt.show()

# AUC-ROC curve Validation Set
logit_roc_auc = roc_auc_score(y_val, y_pred_lr_val)
fpr, tpr, thresholds = roc_curve(y_val, lr.predict_proba(X_val)[:, 1])

plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
plt.title('Receiver Operating Characteristic - Validation', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')
plt.show()

"""### SVM (Support Vector Machine)"""

# Support Vector Machine
svm_model = SVC(kernel = "linear", probability=True, random_state = 42)
svm_model.fit(X_train, y_train)

svm_model.intercept_

svm_model.coef_

svm_model.class_weight_

svm_model.feature_names_in_

svm_model.n_features_in_

# Training Set Estimation
y_pred_svm_train = svm_model.predict(X_train)
y_pred_svm_train

# Validation Set Estimation
y_pred_svm_val = svm_model.predict(X_val)
y_pred_svm_val

# Training Probabilities
y_pred_svm_train_Proba = svm_model.predict_proba(X_train)
y_pred_svm_train_Proba

# Validation Probabilities
y_pred_svm_val_Proba = svm_model.predict_proba(X_val)
y_pred_svm_val_Proba

# Training Classification Report
print("================== SVM Training Classification Report ==================\n")
print(classification_report(y_train, y_pred_svm_train))

# Evaluate the model on the training set
print('=' * 47)
print('======== SVM Training ========')
print('=' * 47)

thresh = 0.5

svm_train_auc, svm_train_accuracy, svm_train_recall, svm_train_precision, svm_train_specificity, svm_train_f1, svm_train_log_loss = print_report(y_train, y_pred_svm_train, thresh)

# Validation Classification Report
print("================== SVM Validation Classification Report ==================\n")
print(classification_report(y_val, y_pred_svm_val))

# Evaluate the model on the validation set
print('=' * 48)
print('======== SVM Validation ========')
print('=' * 48)

thresh = 0.5

svm_val_auc, svm_val_accuracy, svm_val_recall, svm_val_precision, svm_val_specificity, svm_val_f1, svm_val_log_loss = print_report(y_val, y_pred_svm_val, thresh)

# AUC-ROC curve Training Set
logit_roc_auc = roc_auc_score(y_train, y_pred_svm_train)
fpr, tpr, thresholds = roc_curve(y_train, svm_model.predict_proba(X_train)[:, 1])

plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
plt.title('Receiver operating characteristic - SVM - Training', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')
plt.show()

# AUC-ROC curve Validation Set
logit_roc_auc = roc_auc_score(y_val, y_pred_svm_val)
fpr, tpr, thresholds = roc_curve(y_val, svm_model.predict_proba(X_val)[:, 1])

plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
plt.title('Receiver Operating Characteristic - SVM - Validation', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')
plt.show()

"""### KNN (K Nearest Neighbors)"""

# KNN - K Nearest Neighbors.
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)

# Number of features that the estimator expects for subsequent calls to predict or transform.
knn_model.n_features_in_

knn_model.metric

knn_model.n_samples_fit_

knn_model.weights

# Training Set Estimation
y_pred_knn_train = knn_model.predict(X_train)
y_pred_knn_train

# Validation Set Estimation
y_pred_knn_val = knn_model.predict(X_val)
y_pred_knn_val

# Training Probabilities
y_pred_knn_train_Proba = knn_model.predict_proba(X_train)
y_pred_knn_train_Proba

# Validation Probabilities
y_pred_knn_val_Proba = knn_model.predict_proba(X_val)
y_pred_knn_val_Proba

# Training Classification Report
print("================== KNN Training Classification Report ==================\n")
print(classification_report(y_train, y_pred_knn_train))

# Evaluate the model on the training set
print('=' * 47)
print('======== KNN Training ========')
print('=' * 47)

thresh = 0.5

knn_train_auc, knn_train_accuracy, knn_train_recall, knn_train_precision, knn_train_specificity, knn_train_f1, knn_train_log_loss = print_report(y_train, y_pred_knn_train, thresh)

# Validation Classification Report
print("================== KNN Validation Classification Report ==================\n")
print(classification_report(y_val, y_pred_knn_val))

# Evaluate the model on the validation set
print('=' * 48)
print('======== KNN Validation ========')
print('=' * 48)

thresh = 0.5

knn_val_auc, knn_val_accuracy, knn_val_recall, knn_val_precision, knn_val_specificity, knn_val_f1, knn_val_log_loss = print_report(y_val, y_pred_knn_val, thresh)

# AUC-ROC curve Training Set
logit_roc_auc = roc_auc_score(y_train, y_pred_knn_train)
fpr, tpr, thresholds = roc_curve(y_train, knn_model.predict_proba(X_train)[:, 1])

plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
plt.title('Receiver operating characteristic - KNN - Training', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')
plt.show()

# AUC-ROC curve Validation Set
logit_roc_auc = roc_auc_score(y_val, y_pred_knn_val)
fpr, tpr, thresholds = roc_curve(y_val, knn_model.predict_proba(X_val)[:, 1])

plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
plt.title('Receiver Operating Characteristic - KNN - Validation', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')
plt.show()

