# -*- coding: utf-8 -*-
"""bank_marketing_classification_case_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iJQkcSB1NPtVSn9-SDGCSmVKgiCQgJe0
"""

# Commented out IPython magic to ensure Python compatibility.
# Import required packages

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import missingno as msno
from tabulate import tabulate
from scipy import stats
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

from imblearn.over_sampling import RandomOverSampler
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression # logistic regression
from sklearn.svm import SVC # support vector Machine
from sklearn.ensemble import RandomForestClassifier # Random Forest
from sklearn.neighbors import KNeighborsClassifier # KNN
from sklearn.tree import DecisionTreeClassifier # Decision Tree
from sklearn.model_selection import train_test_split # training and testing data split
from sklearn import metrics # accuracy measure
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix # confusion matrix
from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, log_loss, confusion_matrix, classification_report


sns.set_theme(style="whitegrid")

import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

# Load dataset
from google.colab import drive
drive.mount('/content/drive')

path = r'/content/drive/MyDrive/datasets/bank-additional-full.csv'
df=pd.read_csv(path, sep=';', index_col=False)

df.head()

df.tail()

df.info()

df.columns

df.shape

df.dtypes

# Change object data type to categorical data type
df = df.astype({'job':'category', 'marital':'category', 'education':'category', 'default':'category', 'housing':'category', 'loan':'category', 'contact':'category', 'month':'category', 'day_of_week':'category', 'poutcome':'category', 'y':'category'})

df.dtypes

# Change column names
new_column_names = {
    'contact': 'contact_type',
    'duration': 'last_contact_duration',
    'pdays': 'passed_days',
    'poutcome': 'previous_outcome',
    'emp.var.rate': 'employment_variation_rate',
    'cons.price.idx': 'consumer_price_index',
    'cons.conf.idx': 'consumer_confidence_index',
    'nr.employed': 'number_of_employees'
}

# Rename the columns
df.rename(columns=new_column_names, inplace=True)

# Create a column called has_term_deposit which is going to represent 0 for the negative class and 1 for the positive class.
df['has_term_deposit'] = (df.y == 'yes').astype('int')

"""### Data Cleaning"""

# Create a new column called age_band
df['age_band'] = pd.cut(df['age'], [1, 20, 30, 40, 50, 60, 70, 80, 90, 100],
                              labels=['1-20', '20-30', '30-40', '40-50','50-60','60-70','70-80', '80-90','90-100'])

# Replace any `unknown` value with other values of job randomly.
job_values = ['admin.', 'blue-collar', 'technician', 'services', 'management', 'retired', 'entrepreneur', 'self-employed', 'housemaid', 'unemployed', 'student']

# Replace 'unknown' values with a random job from the specified list
df['job'] = np.where(df['job'] == 'unknown', np.random.choice(job_values, size=len(df)), df['job'])

# Replace any `unknown` value with other values of marital randomly.
marital_values = ['married', 'single', 'divorced']

# Replace 'unknown' values with a random marital from the specified list
df['marital'] = np.where(df['marital'] == 'unknown', np.random.choice(marital_values, size=len(df)), df['marital'])

df['education'] = df['education'].replace('basic.4y', 'primary')

df['education'] = df['education'].replace('basic.6y', 'secondary')

df['education'] = df['education'].replace('basic.9y', 'tertiary')

df['education'] = df['education'].replace('high.school', 'high_school')

df['education'] = df['education'].replace('university.degree', 'university_degree')

df['education'] = df['education'].replace('professional.course', 'professional_course')

# Replace any `unknown` value with other values of education randomly.
education_values = ['university_degree', 'high_school', 'tertiary', 'professional_course', 'primary', 'secondary', 'illiterate']

# Replace 'unknown' values with a random education from the specified list
df['education'] = np.where(df['education'] == 'unknown', np.random.choice(education_values, size=len(df)), df['education'])

# Replace any `unknown` value with other values of default randomly.
default_values = ['yes', 'no']

# Replace 'unknown' values with a random default from the specified list
df['default'] = np.where(df['default'] == 'unknown', np.random.choice(default_values, size=len(df)), df['default'])

# Replace any `unknown` value with other values of housing randomly.
housing_values = ['yes', 'no']

# Replace 'unknown' values with a random housing from the specified list
df['housing'] = np.where(df['housing'] == 'unknown', np.random.choice(housing_values, size=len(df)), df['housing'])

# Replace any `unknown` value with other values of loan randomly.
loan_values = ['yes', 'no']

# Replace 'unknown' values with a random loan from the specified list
df['loan'] = np.where(df['loan'] == 'unknown', np.random.choice(loan_values, size=len(df)), df['loan'])

df['passed_days'] = df['passed_days'].replace(999, -1)

"""- passed_days: number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted)

- IMPORTANT NOTE: It is worth mentioning here that "was not previously contacted" is only in the context of a previous campaign and does not mean they were not previously contacted at all.
"""

"""
The passed_days data indicates how many times the customer has been contacted before.

- if the passed_days = 0, it indicates that it has not been contacted before.
- if the passed_days = 1, it indicates that it was contacted earlier.
"""

def passed_days_work(pdays):
    if(pdays == -1):
        return(0)
    elif(pdays >= 0):
        return(1)


df['new_passed_days'] = df['passed_days'].apply(passed_days_work)

# Replace any `nonexistent` value with other values of previous_outcome randomly.
previous_outcome_values = ['success', 'failure']

# Replace 'nonexistent' values with a random previous_outcome from the specified list
df['previous_outcome'] = np.where(df['previous_outcome'] == 'nonexistent', np.random.choice(previous_outcome_values, size=len(df)), df['previous_outcome'])

# Check the duplicated rows
df.duplicated(keep=False).sum()

# Print duplicated values

"""
Duplicated rows can introduce bias and skew the analysis results. By removing duplicated rows, analysts can ensure the accuracy and integrity of the data, leading to more reliable insights and conclusions.
"""

duplicated_rows = df[df.duplicated(keep=False)]  # keep=False marks all occurrences as True
duplicated_rows

"""
For duplicated values, we can do this:
    - Review and Understand.
    - Address Data Entry Issues.
    - Aggregate Duplicates.
    - Flag Duplicates.
    - Handle Partial Duplicates.
    - Identify and Remove Duplicates.
"""

df = df.drop_duplicates()

df.duplicated(keep=False).sum()

# Check the missing values
df.isnull().sum()

# Plot the missing values
msno.bar(df)

# Plot the missing values
msno.matrix(df)

df.shape

# Explore unique values and their counts in a specified column

def explore_column(df, column_name):

    unique_values = df[column_name].unique()
    value_counts = df[column_name].value_counts()

    return unique_values, value_counts

unique_values, value_counts = explore_column(df, 'age')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'job')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'marital')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'education')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'default')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'housing')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'loan')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'contact_type')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'month')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'day_of_week')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'last_contact_duration')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'campaign')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'passed_days')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'previous')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'previous_outcome')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'employment_variation_rate')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'consumer_price_index')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'consumer_confidence_index')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'euribor3m')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'number_of_employees')

print("Unique Values:")
print(unique_values)

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'has_term_deposit')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'age_band')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

unique_values, value_counts = explore_column(df, 'new_passed_days')

print("Unique Values:")
print(sorted(unique_values))

print("\n", '-' * 40, "\n")

print("Value Counts:")
print(value_counts)

# Plot the (target) customers subscriptions to a term deposit

plt.figure(figsize=(20, 10))

counts = df['y'].value_counts()
labels = counts.index

plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=90, colors=['skyblue', 'lightcoral'])
plt.title('Distribution of customers who are subscribed or unsubscribed to a term deposit?', fontsize=14, fontweight='bold')
plt.show()

# Plot customers subscriptions to a term deposit
fig = px.pie(df, names='y', title='Distribution of customers who are subscribed or unsubscribed to a term deposit?')
fig.show()

"""### ðŸ‘†ðŸ» The dataset is imbalanced."""

df[list(df.columns)[:12]].sample(10)

df[list(df.columns)[12:]].sample(10)

# Plot outliers for numerical variables using box plot

def plot_numerical_vars_with_outliers(df, column_name):
    plt.rcParams['axes.facecolor'] = '#f8fafc'
    plt.figure(figsize=(18, 10))

    sns.boxplot(x=df[column_name], palette='Paired')
    plt.title('Box Plot of Numerical Variables with Outliers')
    plt.show()

# Plot outliers for numerical variables using box plot (Plotly package)
def plot_numerical_vars_with_outliers_plotly(df, column_name, background_color='#f8fafc'):
    fig = go.Figure()

    # Add box plot
    fig.add_trace(go.Box(x=df[column_name], marker=dict(color='#f43f5e')))

    # Add mean as dotted line
    mean_value = df[column_name].mean()
    fig.add_trace(go.Scatter(x=[mean_value, mean_value], y=[0, 1], mode='lines', line=dict(dash='dash', color='black'), name='Mean'))

    # Update layout
    fig.update_layout(
        title=f'Box Plot of Numerical Variable {column_name} with Outliers',
        paper_bgcolor=background_color
    )

    fig.show()

# Plot distribution of categorical variables.
def plot_categorical_vars_distribution(df, column_name, background_color='#f8fafc', palette='pastel'):
    # Set background color
    plt.rcParams['axes.facecolor'] = background_color

    # Calculate percentage values
    total_count = len(df)
    percentage_values = (df[column_name].value_counts() / total_count) * 100

    # Create count plot
    plt.figure(figsize=(18, 10))
    ax = sns.countplot(x=df[column_name], palette=palette)

    # Add percentage labels and borders
    for p in ax.patches:
        percentage = f'{p.get_height() / total_count * 100:.2f}%'
        x = p.get_x() + p.get_width() / 2
        y = p.get_height()
        ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=10, color='black')

    # Customize the plot
    plt.title(f'Count Plot of Categorical Variable - {column_name}')
    plt.xticks(rotation=90)
    sns.despine(left=True, bottom=True)  # Remove spines on the left and bottom

    # Show the plot
    plt.show()

# Plot distribution of categorical variables (Using Plotly)
def plot_categorical_vars_distribution_plotly(df, column_name, background_color='#f8fafc'):
    # Calculate percentage values
    total_count = len(df)
    percentage_values = (df[column_name].value_counts() / total_count) * 100

    # Create a DataFrame with count and percentage values
    count_df = df[column_name].value_counts().reset_index()
    count_df.columns = [column_name, 'count']
    count_df['percentage'] = (count_df['count'] / total_count) * 100

    # Create a bar plot using Plotly
    fig = px.bar(count_df, x=column_name, y='count', color=column_name,
                 labels={'x': column_name, 'y': 'Count'}, title=f'Count Plot of Categorical Variable - {column_name}',
                 color_discrete_sequence=px.colors.qualitative.Pastel)

    # Add percentage labels and borders
    for bar in fig.data:
        percentage = f'{count_df[count_df[column_name] == bar.name]["percentage"].values[0]:.2f}%'
        bar.update(text=percentage, hoverinfo='text')

    # Customize the layout
    fig.update_layout(
        paper_bgcolor=background_color,
        xaxis_tickangle=-45
    )

    # Show the plot
    fig.show()

plot_numerical_vars_with_outliers(df, 'age')

plot_numerical_vars_with_outliers_plotly(df, 'age')

plot_numerical_vars_with_outliers(df, 'last_contact_duration')

plot_numerical_vars_with_outliers_plotly(df, 'last_contact_duration', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'campaign')

plot_numerical_vars_with_outliers_plotly(df, 'campaign', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'passed_days')

plot_numerical_vars_with_outliers_plotly(df, 'passed_days', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'new_passed_days')

plot_numerical_vars_with_outliers_plotly(df, 'new_passed_days', '#E6ECF5')

plot_numerical_vars_with_outliers(df, 'previous')

plot_numerical_vars_with_outliers_plotly(df, 'previous', '#E6ECF5')

"""### Handling outliers"""

# Create a copy of the dataset
df2 = df.copy()

# Using Z-scores [Count the number of outliers in a specific column of a dataset using Z-scores]
def detect_count_outliers(df, column_name, threshold=3):

    # Calculate Z-scores for the specified column
    z_scores = stats.zscore(df[column_name])

    # Identify outliers
    outliers_df = df[abs(z_scores) > threshold]

    # Count the number of outliers
    num_outliers = len(outliers_df)

    print(f"Number of outliers in '{column_name}': {num_outliers}")

    return num_outliers, outliers_df

# Handle outliers by replacing them with the median.
def handle_outliers_with_median(df, column_name, threshold=3):

    # Calculate Z-scores for the specified column
    z_scores = stats.zscore(df[column_name])

    # Replace outliers with the median
    df.loc[abs(z_scores) > threshold, column_name] = df[column_name].median()

    return df

num_outliers, outliers_df = detect_count_outliers(df2, 'age', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'age', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'age', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'age')

plot_numerical_vars_with_outliers_plotly(df2, 'age')

num_outliers, outliers_df = detect_count_outliers(df2, 'last_contact_duration', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'last_contact_duration', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'last_contact_duration', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'last_contact_duration')

plot_numerical_vars_with_outliers_plotly(df2, 'last_contact_duration', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'campaign', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'campaign', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'campaign', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'campaign')

plot_numerical_vars_with_outliers_plotly(df2, 'campaign', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'passed_days', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'passed_days', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'passed_days', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'passed_days')

plot_numerical_vars_with_outliers_plotly(df2, 'passed_days', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'previous', threshold=3)
num_outliers
outliers_df

df2 = handle_outliers_with_median(df2, 'previous', threshold=3)

num_outliers, outliers_df = detect_count_outliers(df2, 'previous', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'previous')

plot_numerical_vars_with_outliers_plotly(df2, 'previous', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'employment_variation_rate', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'employment_variation_rate')

plot_numerical_vars_with_outliers_plotly(df2, 'employment_variation_rate', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'consumer_price_index', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'consumer_price_index')

plot_numerical_vars_with_outliers_plotly(df2, 'consumer_price_index', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'consumer_confidence_index', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'consumer_confidence_index')

plot_numerical_vars_with_outliers_plotly(df2, 'consumer_confidence_index', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'euribor3m', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'euribor3m')

plot_numerical_vars_with_outliers_plotly(df2, 'euribor3m', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'number_of_employees', threshold=3)
num_outliers
outliers_df

plot_numerical_vars_with_outliers(df2, 'number_of_employees')

plot_numerical_vars_with_outliers_plotly(df2, 'number_of_employees', '#E6ECF5')

num_outliers, outliers_df = detect_count_outliers(df2, 'has_term_deposit', threshold=3)
num_outliers
outliers_df

# See the correlations between categorical variables
def plot_categorical_count(df, x_column, hue_column, title, palette='Paired'):

    plt.figure(figsize=(20, 10))
    sns.countplot(x=x_column, hue=hue_column, data=df, palette=palette)

    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel(x_column)
    plt.ylabel('Count')
    plt.legend(title=hue_column, loc='upper right')

    plt.show()

# See the correlations between categorical variables (Using Plotly)
def plotly_categorical_count(df, x_column, hue_column, title, color_discrete_sequence=None):
    # Create a dataframe with counts and percentages
    count_df = df.groupby([x_column, hue_column]).size().reset_index(name='count')
    total_counts = count_df.groupby(x_column)['count'].transform('sum')
    count_df['percentage'] = (count_df['count'] / total_counts) * 100

    # Create the bar plot
    fig = px.bar(count_df, x=x_column, y='count', color=hue_column,
                 title=title, color_discrete_sequence=color_discrete_sequence)

    # Add borders and percentage labels
    for bar in fig.data:
        bar.update(marker_line_color='black', marker_line_width=1, hoverinfo='y+text',
                   text=count_df[count_df[hue_column] == bar.name]['percentage'].round(2).astype(str) + '%')

    fig.update_layout(
        xaxis_title=x_column,
        yaxis_title='Count',
        legend_title=hue_column,
        barmode='group',
        bargap=0.1,
        showlegend=True,
        legend=dict(x=1.02, y=1)
    )

    fig.show()

df2.groupby(['job','y'])['y'].count()

pd.crosstab(df2.y, df2.job, margins=True).style.background_gradient(cmap='summer_r')

plot_categorical_count(df2[['job', 'y']], x_column='job', hue_column='y', title='Count Plot of Job vs. Subscription', palette='PuRd')

# Plotly
plotly_categorical_count(df2[['job', 'y']], x_column='job', hue_column='y', title='Count Plot of Job vs. Subscription')

df2.groupby(['marital','y'])['y'].count()

pd.crosstab(df2.y, df2.marital, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.job, df2.y], df.marital, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between job, subscription and marital status

cross_tab_df = pd.crosstab([df2['job'], df2['y']], df2['marital'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Job, Subscription, and Marital Status', fontweight='bold', fontsize=16)
plt.xlabel('Marital Status', fontweight='bold', fontsize=14)
plt.ylabel('Job and Subscription', fontweight='bold', fontsize=14)
plt.show()

plot_categorical_count(df2[['marital', 'y']], x_column='marital', hue_column='y', title='Count Plot of Marital status vs. Subscription', palette='Reds')

# Plotly
plotly_categorical_count(df2[['marital', 'y']], x_column='marital', hue_column='y', title='Count Plot of Marital status vs. Subscription')

df2.groupby(['education','y'])['y'].count()

pd.crosstab(df2.y, df2.education, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.job, df2.y], df2.education, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between job, subscription and education

cross_tab_df = pd.crosstab([df2['job'], df2['y']], df2['education'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Job, Subscription, and Education', fontweight='bold', fontsize=16)
plt.xlabel('Education', fontweight='bold', fontsize=14)
plt.ylabel('Job and Subscription', fontweight='bold', fontsize=14)
plt.show()

plot_categorical_count(df2[['education', 'y']], x_column='education', hue_column='y', title='Count Plot of Education vs. Subscription', palette='PuRd')

# Plotly
plotly_categorical_count(df2[['education', 'y']], x_column='education', hue_column='y', title='Count Plot of Education vs. Subscription')

df2.groupby(['default','y'])['y'].count()

pd.crosstab(df2.y, df2.default, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.age, df2.y], df2.default, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between age, subscription and default

cross_tab_df = pd.crosstab([df2['age'], df2['y']], df2['default'], margins=True)

plt.figure(figsize=(20, 25))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Age, Subscription, and Default(has credit in default?)', fontweight='bold', fontsize=16)
plt.xlabel('Education', fontweight='bold', fontsize=14)
plt.ylabel('Job and Subscription', fontweight='bold', fontsize=14)
plt.show()

plot_categorical_count(df2[['default', 'y']], x_column='default', hue_column='y', title='Count Plot of -Has credit in default?- vs. Subscription', palette='Set3')

# Plotly
plotly_categorical_count(df2[['default', 'y']], x_column='default', hue_column='y', title='Count Plot of - Has credit in default? - vs. Subscription')

df2.groupby(['housing','y'])['y'].count()

pd.crosstab(df2.y, df2.housing, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.marital, df2.y], df2.housing, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between marital, subscription and housing

cross_tab_df = pd.crosstab([df2['marital'], df2['y']], df2['housing'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Marital, Subscription, and Housing (has housing loan?)', fontweight='bold', fontsize=16)
plt.xlabel('Housing', fontweight='bold', fontsize=14)
plt.ylabel('Marital and Subscription', fontweight='bold', fontsize=14)
plt.show()

plot_categorical_count(df2[['housing', 'y']], x_column='housing', hue_column='y', title='Count Plot of Housing loan vs. Subscription', palette='Wistia')

# Plotly
plotly_categorical_count(df2[['housing', 'y']], x_column='housing', hue_column='y', title='Count Plot of Housing loan vs. Subscription')

df2.groupby(['loan','y'])['y'].count()

pd.crosstab(df2.y, df2.loan, margins=True).style.background_gradient(cmap='summer_r')

pd.crosstab([df2.loan, df2.y], df2.marital, margins=True).style.background_gradient(cmap='summer_r')

# Plot the correlation between loan, subscription and marital status

cross_tab_df = pd.crosstab([df2['loan'], df2['y']], df2['marital'], margins=True)

plt.figure(figsize=(20, 10))
heatmap = sns.heatmap(cross_tab_df, annot=True, cmap='summer_r', fmt='d', cbar=True, linewidths=.5, linecolor='black')

# Bold text for feature names
for _, spine in heatmap.spines.items():
    spine.set_visible(True)

heatmap.set_title('Cross-tabulation of Loan (has personal loan?), Subscription, and Marital', fontweight='bold', fontsize=16)
plt.xlabel('Marital', fontweight='bold', fontsize=14)
plt.ylabel('Loan and Subscription', fontweight='bold', fontsize=14)
plt.show()

plot_categorical_count(df2[['loan', 'y']], x_column='loan', hue_column='y', title='Count Plot of Personal loan vs. Subscription', palette='brg')

# Plotly
plotly_categorical_count(df2[['loan', 'y']], x_column='loan', hue_column='y', title='Count Plot of Personal loan vs. Subscription')

# Select the specified features
numeric_features = df2[['age', 'last_contact_duration', 'campaign', 'passed_days', 'previous',
            'employment_variation_rate', 'consumer_price_index', 'consumer_confidence_index',
            'euribor3m', 'number_of_employees', 'new_passed_days', 'has_term_deposit']]

# Calculate the correlation matrix
correlation_matrix = numeric_features.corr()

# Set up the matplotlib figure
plt.figure(figsize=(20, 10))

# Plot the correlation matrix using a heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

# Display the plot
plt.title('Correlation Matrix', fontweight='bold', fontsize=14)
plt.show()

numeric_features = df2[['age', 'last_contact_duration', 'campaign', 'passed_days', 'previous',
            'employment_variation_rate', 'consumer_price_index', 'consumer_confidence_index',
            'euribor3m', 'number_of_employees', 'new_passed_days', 'has_term_deposit']]

# Calculate the correlation matrix
correlation_matrix = numeric_features.corr()

# Create a Plotly figure for the correlation matrix
fig = px.imshow(correlation_matrix,
                labels=dict(x="Features", y="Features", color="Correlation"),
                x=numeric_features.columns,
                y=numeric_features.columns,
                color_continuous_scale='blues',  # Choose an appropriate color scale
                title='Correlation Matrix')

# Customize the layout
fig.update_layout(width=1000, height=800)

# Show the figure
fig.show()

# Plot a correlation matrix heatmap between one numeric feature and target
def plot_correlation_heatmap(df, features, target, cmap='coolwarm', figsize=(10, 8)):
    df_selected = df[features + [target]]

    # Calculate correlation matrix
    correlation_matrix = df_selected.corr()

    # Set up the matplotlib figure
    plt.figure(figsize=figsize)

    # Create a heatmap using seaborn
    sns.heatmap(correlation_matrix, annot=True, cmap=cmap, fmt='.2f', linewidths=0.5)

    # Set the title
    plt.title(f'Correlation Matrix Heatmap - {target} vs {", ".join(features)}')

    # Show the plot
    plt.show()

# Plot a correlation matrix heatmap between one numeric feature and target

def plot_correlation_heatmap_plotly(df, features, target, colorscale='Viridis'):
    # Select relevant columns
    df_selected = df[features + [target]]

    # Calculate correlation matrix
    correlation_matrix = df_selected.corr()

    # Create a heatmap using Plotly
    heatmap = go.Figure(data=go.Heatmap(
        z=correlation_matrix.values,
        x=correlation_matrix.columns,
        y=correlation_matrix.index,
        colorscale=colorscale,
        colorbar=dict(title='Correlation'),
    ))

    # Customize the layout
    heatmap.update_layout(
        title=f'Correlation Matrix Heatmap - {target} vs {", ".join(features)}',
        xaxis=dict(title='Variable'),
        yaxis=dict(title='Variable'),
    )

    # Show the plot
    heatmap.show()

plot_correlation_heatmap(df2, features=['age'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['age'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['last_contact_duration'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['last_contact_duration'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['campaign'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['campaign'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['passed_days'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['passed_days'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['previous'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['previous'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['employment_variation_rate'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['employment_variation_rate'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['consumer_price_index'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['consumer_price_index'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['consumer_confidence_index'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['consumer_confidence_index'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['euribor3m'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['euribor3m'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['number_of_employees'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['number_of_employees'], target='has_term_deposit')

plot_correlation_heatmap(df2, features=['new_passed_days'], target='has_term_deposit')

plot_correlation_heatmap_plotly(df2, features=['new_passed_days'], target='has_term_deposit')

plot_categorical_vars_distribution(df2, 'job')

plot_categorical_vars_distribution_plotly(df2, 'job', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'marital')

plot_categorical_vars_distribution_plotly(df2, 'marital', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'education')

plot_categorical_vars_distribution_plotly(df2, 'education', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'default')

plot_categorical_vars_distribution_plotly(df2, 'default', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'housing')

plot_categorical_vars_distribution_plotly(df2, 'housing', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'loan')

plot_categorical_vars_distribution_plotly(df2, 'loan', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'contact_type')

plot_categorical_vars_distribution_plotly(df2, 'contact_type', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'month')

plot_categorical_vars_distribution_plotly(df2, 'month', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'day_of_week')

plot_categorical_vars_distribution_plotly(df2, 'day_of_week', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'previous_outcome')

plot_categorical_vars_distribution_plotly(df2, 'previous_outcome', '#E6ECF5')

plot_categorical_vars_distribution(df2, 'y')

# Distribution of the Target
plot_categorical_vars_distribution_plotly(df2, 'y', '#E6ECF5')

# Correlation matrix

fig, ax = plt.subplots(figsize=(18, 10))

df_corr = numeric_features.corr(method='pearson')
mask = np.zeros_like(df_corr)
mask[np.triu_indices_from(mask)] = True

sns.heatmap(data=df_corr, vmin=-1, vmax=1, mask=mask,
            cmap=sns.color_palette('coolwarm', as_cmap=True),
            annot=True, fmt='.2f', linewidths=.5, ax=ax)

ax.set_xticklabels(ax.get_xticklabels(), rotation=65)

df2.describe().T

print('=' * 60)
print('=' * 20, 'Summary statistics', '=' * 20)
print('=' * 60)

# Summary statistics
summary_stats = numeric_features.describe()

# Mean
mean_values = numeric_features.mean()

# Median
median_values = numeric_features.median()

# Standard Deviation
std_dev_values = numeric_features.std()

# Correlation matrix
correlation_matrix = numeric_features.corr()

# Display the results using tabulate
print("\nSummary Statistics:")
print(tabulate(summary_stats, headers='keys', tablefmt='pretty'))

print("\nMean Values:")
print(tabulate(mean_values.to_frame(), headers='keys', tablefmt='pretty'))

print("\nMedian Values:")
print(tabulate(median_values.to_frame(), headers='keys', tablefmt='pretty'))

print("\nStandard Deviation:")
print(tabulate(std_dev_values.to_frame(), headers='keys', tablefmt='pretty'))

print("\nCorrelation Matrix:")
print(tabulate(correlation_matrix, headers='keys', tablefmt='pretty'))

df2.isnull().sum()

df2.shape

# Commented out IPython magic to ensure Python compatibility.
# To see all the variables created globally across the notebook.
# %whos

"""### Convert Categorical features to Numeric features"""

df2.dtypes

LE=LabelEncoder()
cat_var=['job', 'marital', 'education', 'default', 'housing',
         'loan', 'contact_type', 'month', 'day_of_week',
         'previous_outcome', 'y', 'age_band']

for i in cat_var:
  df2[i]=LE.fit_transform(df2[i])

# Split the Data into Validation and Training Sets
X = df2.drop(['has_term_deposit', 'y'], axis=1)
y = df2['has_term_deposit']

sc = StandardScaler()
sc.fit_transform(X)

# Handling the imbalanced data by Resampling
sm = SMOTE(random_state = 42)
X_resampled, y_resampled = sm.fit_resample(X, y)

# This code first splits the dataset into a training set (70%) and a temporary set (30%). Then, it splits the temporary set into testing and validation sets, resulting in the desired sizes for each set.

# The final splits:
# Training set size = 70%
# Testing set size = 15%
# Validation set size = 15%

# Dataset splitting into training and temporary set (combining testing and validation)
X_train_temp, X_temp, y_train_temp, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Now, split the temporary set into testing and validation

# testing
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# validation
X_train, X_val, y_train, y_val = train_test_split(X_train_temp, y_train_temp, test_size=0.5, random_state=42)

X_train.shape

X_test.shape

X_val.shape

y_train.shape

y_test.shape

y_val.shape

"""Model Selection
* Logistic regression.
* SVM - Support Vector Machine.
* KNN - K Nearest Neighbors.
* Decision tree.
* Random forest.
"""

def calc_popularity(y_actual):
  return (sum(y_actual)/len(y_actual))

# Calculates specificity
def calc_specificity(y_actual, y_pred, thresh):
  return sum((y_pred < thresh) & (y_actual == 0)) / sum(y_actual ==0)

# Evaluation of model performance
def print_report(y_actual, y_pred, thresh):
  auc = roc_auc_score(y_actual, y_pred)
  accuracy = accuracy_score(y_actual, (y_pred > thresh))
  recall = recall_score(y_actual, (y_pred > thresh))
  precision = precision_score(y_actual, (y_pred > thresh))
  specificity = calc_specificity(y_actual, y_pred, thresh)
  f1 = 2 * (precision * recall) / (precision + recall)
  logloss = log_loss(y_actual, y_pred)

  tn, fp, fn, tp = confusion_matrix(y_actual, (y_pred > thresh)).ravel()

  print('AUC: %.3f' % auc)
  print('Accuracy: %.3f' % accuracy)
  print('Recall: %.3f' % recall)
  print('Precision: %.3f' % precision)
  print('Specificity: %.3f' % specificity)
  print('Popularity: %.3f' % calc_popularity(y_actual))
  print('F1: %.3f' % f1)
  print('Log Loss: %.3f' % logloss)

  print('-'*20)

  confusion_matrix_table = tabulate([['True Negative', tn], ['False Positive', fp], ['False Negative', fn], ['True Positive', tp]], headers=['Metric', 'Value'], tablefmt='grid')

  print('---------- Confusion Matrix ----------\n')
  print(confusion_matrix_table)

  print(' ')
  return auc, accuracy, recall, precision, specificity, f1, logloss

# Since we balanced our training data, let's set our threshold at 0.5 to label a predicted sample as positive.
thresh = 0.5

"""### Logistic Regression"""

# Logistic regression
lr = LogisticRegression(random_state=42)
lr.fit(X_train, y_train)

lr.random_state

# Training Set Estimation
y_pred_lr_train = lr.predict(X_train)
y_pred_lr_train

# Validation Set Estimation
y_pred_lr_val = lr.predict(X_val)
y_pred_lr_val

# Training Probabilities
y_pred_lr_train_Proba = lr.predict_proba(X_train)
y_pred_lr_train_Proba

# Validation Probabilities
y_pred_lr_val_Proba = lr.predict_proba(X_val)
y_pred_lr_val_Proba

# Training Classification Report
print("================== Logistic Regression Training Classification Report ==================\n")
print(classification_report(y_train, y_pred_lr_train))

# Evaluate the model on the training set
print('=' * 47)
print('======== Logistic Regression Training ========')
print('=' * 47)

thresh = 0.5

lr_train_auc, lr_train_accuracy, lr_train_recall, lr_train_precision, lr_train_specificity, lr_train_f1, lr_train_log_loss = print_report(y_train, y_pred_lr_train, thresh)

# Evaluate the model on the validation set
print('=' * 48)
print('======== Logistic Regression Validation ========')
print('=' * 48)

thresh = 0.5
lr_val_auc, lr_val_accuracy, lr_val_recall, lr_val_precision, lr_val_specificity, lr_val_f1, lr_val_log_loss = print_report(y_val, y_pred_lr_val, thresh)

# AUC-ROC curve Training Set
logit_roc_auc = roc_auc_score(y_train, y_pred_lr_train)
fpr, tpr, thresholds = roc_curve(y_train, lr.predict_proba(X_train)[:, 1])
plt.figure(figsize=(20, 10))

plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')

plt.title('Receiver operating characteristic - Logistic Regression - Training', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")
# plt.savefig('Log_ROC')

plt.show()

# AUC-ROC curve Validation Set
logit_roc_auc = roc_auc_score(y_val, y_pred_lr_val)
fpr, tpr, thresholds = roc_curve(y_val, lr.predict_proba(X_val)[:, 1])

plt.figure(figsize=(20, 10))
plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')

plt.title('Receiver Operating Characteristic - Logistic Regression - Validation', fontsize=20, fontweight='bold')
plt.legend(loc="lower right")

# plt.savefig('Log_ROC')
plt.show()

"""### SVM (Support Vector Machine)"""

